---
title: "Lab 5: Hierarchical models and likelihood profiles"
output: html_document
date: "2023-07-07"
author: "Kyle L. Wilson"
---

```{r setup}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_knit$set(root.dir = rprojroot::find_rstudio_root_file())
library(nlme)
library(stats4)
library(ggplot2)
```

## What are fixed and random effects

*Fixed Effects* are parameters associated with the entire population or with certain repeatable levels of experimental units. - For example, what is the expected body size of a yelloweye rockfish.

*Random Effects* are parameters associated with individual experimental units drawn at random from a population.
- For example, what is the expected body size of the 23rd population of yelloweye rockfish, as chosen from a database.

A mixed effects model is a model that has both *fixed effects* and *random effects*. Often the same random effect is assigned to observations sharing a common grouping factor (or other classifying factor). Consider length-at-age data for several (randomly selected) populations of a species. A random factor could be stock and the observations are length-at-age data. Random effects can be thought of as ways of modelling the covariance structure of the data.

Classical (frequentist) and Bayesian methods can be used to estimate the parameters of mixed effects models. This lab and homework will cover Frequentist. Lab 6 will cover their implementation in Stan.

We selected six streams (at random) and measure fish densities at each stream, each surveyed three times. We can then ask the following questions: 
-What is the density in a typical stream?
-What is the variation in density among streams?
-What is the variation in density estimates within a stream?


```{r read the data}
Streams <- readRDS(file="Data/streams.rds")
colnames(Streams) <- c("Stream","true_density","Density")
```

### Fixed effects model
First, let's fit a relatively simple intercept-only model which speaks to the fish density in a typical stream.

```{r lm version 1, echo=FALSE}
 print("Linear Fixed Effects Model")
 lm1 <- lm(Density~1,data=Streams)
 boxplot(split(lm1$residuals,Streams$Stream),ylab="Residual",xlab="Stream",csi=0.2)
 abline(0,0,lwd=3)
 print(summary(lm1))
 print(paste("AIC ",AIC(lm1)))
```

Note that this analysis attributed all the error to within-stream variation. However, our residual plots by stream (as a grouping factor) illustrates that there is between-stream variation in density but we have no way to make any inference or insight on it.

Let's see if we can ask how do streams vary instead? Fit a linear mixed effects model (and drop the intercept using the `-1` term in `lm()`)
```{r lm version 2, echo=FALSE}
print("Linear Fixed Effects Model, with stream-effect") 
lm2 <- lm(Density~as.factor(Stream)-1,data=Streams)   
 boxplot(split(lm2$residuals,Streams$Stream),ylab="Residual",xlab="Stream",csi=0.2)
 abline(0,0,lwd=3)
 print(summary(lm2))
 print(paste("AIC ",AIC(lm2)))
```

Well, good news. The residual pattern has been removed, but now we have no overall mean and no way to comment on between-stream variance, i.e. we cannot say anything about the population of streams. From here, we will fit a mixed-effects model. This can be done with a few different packages in R including `nlme`, `lme4`, `glmmTMB`, and more! In a Bayesian context, this can be fit using `brms` (which you may wish to try at some point for your assignments).

```{r lme, echo=FALSE}
 lm3 <- lme(fixed = Density ~ 1,data=Streams,random = ~ 1 | Stream) 
 boxplot(split(lm3$residuals,Streams$Stream),ylab="Residual",xlab="Stream",csi=0.2)
 abline(0,0,lwd=3)
 print(summary(lm3))
```
Now, we've taken care of the strong patterning in the residuals and we can now comment on an overall mean and between-stream variance. What are the estimates of the stream-specifics?

```{r extract randome effects}
plot(as.numeric(unique(Streams$true_density)),coef(lm3)[[1]],xlab="True value",ylab="Random effects") # extract the stream-level effects (fixed effect + random effects) and compare to true values
abline(b=1,a=0) # add a 1:1 line for comparison
```

### Hierarchical model: what's under the hood

We are going to construct a numerical algorithm, using Simpson's 1/3 rule (see here: https://en.wikipedia.org/wiki/Simpson%27s_rule#Simpson's_1/3_rule), to approximate the integral of the random effects conditional on the estimates for the fixed effects. To do this, we will:
- create a 1st function that calculates the likelihood inside the step of the integral
- create a 2nd function that applies the Simpson's Rule that to function 1 to look over possibles values of the random effects $b_i$, from -5 to 5 and calls *function 1*
- create a 3rd function that provides the joint likelihood for the model and calls *function 2*
- create a 4th function that maximizes the likelihood, using the `mle()` function in R that calls *function 3*.

```{r calculate likelihood inside the integral, echo=FALSE}
norm.lik = function(bi,sig_e,sig_b,beta,stream)
{
  nobs<-3 #number of observations per stream
  lik.a<-lik.fix<-rep(NA,nobs)
  dat<-subset(Streams,Streams$Stream==stream) # subset the data to the group in question
  lik.random<-1/sqrt(2*pi)*exp(-(bi^2)/2) # Question: what does this statement imply?
  for(k in 1:nobs)
  {
    lik.fix[k]<-1/sqrt(2*pi*sig_e^2)*exp(-((dat$Density[k]-beta-bi*sig_b)^2)/(2*sig_e^2)) # likelihood, conditional on fixed effects plus 
  }
  lik.fix.a<-prod(lik.fix) # remember: joint likelihoods are the PRODUCT of all likelihoods (or, alternatively, the joint log-likelihood is the SUM of all log-likelihoods)
  lik.a<-lik.random*lik.fix.a # total likelihood of this slice of the integral is the product of the fixed effect and the random effect likelihoods
  return(lik.a)
}
```

Here is *Function 2*, which applies Simpson's 1/3 Rule and calls *Function 1* to integrate the likelihood for random effects across potential values for the random effect $b_i$ (bounded from -5 to 5) that changed by a sequence of small steps. Simpson’s rule allows us to calculate a full likelihood for the parameter estimates (the fixed effects) that accounted for the likelihood of the random effect of stream *i*. 

```{r apply simpsons rule in function 2}
simpson  <-  function(lowerb, upperb, nbins, stream,beta, sig_e, sig_b) 
{
  step <- (upperb-lowerb)/nbins
  b_vec <- seq(lowerb, upperb, by=step)
  frac_vec <- c(1/3, rep(c(4/3, 2/3), times=((nbins+1)-3)/2), 4/3, 1/3)
  sum_comp <- vector(length=length(b_vec))
  for(i in 1:length(b_vec)){
    sum_comp[i] <- frac_vec[i]*norm.lik(bi=b_vec[i], sig_e=sig_e, sig_b=sig_b, beta=beta, stream=stream)
  }
  full_like <- sum(sum_comp)*step
  return(full_like)
}
```
We should be somewhat comfortable with the below function: we pass some parameters to a function and call *Function 2* (which calls *Function 1*) to calculate the joint likelihood.
```{r likelihood}
loglike=function(l_sig_e,l_sig_b,beta)
{
  sig_e <- exp(l_sig_e)
  sig_b<-exp(l_sig_b)
  stream <- c(1,2,3,4,5,6)
  like.str <- vector(length=length(stream))
  for (i in stream)
  {
    like.str[i] <- simpson(lowerb=lowerb, upperb=upperb, nbins=nbins, sig_e=sig_e, sig_b=sig_b, beta=beta, stream=stream[i])
  }
  nll <- (-1)*sum(log(like.str))
  return(nll)
}
```

Now let's optimize! Rather than `optim()` we will use a similar package called `mle`, which automatically provides some standard errors based on the Hessian matrix from Lab 2. Note that `mle()` is a function, which calls `loglike()`, which calls `simpson()`, which calls `norm.lik()`. It's why hierarchical models are complicated!

Below, we will the number of bins for our integral and the lower- and upper-bounds to integrate across.
```{r MLE fit}
lowerb <- -5 # the lower bound of the integral
upperb <- 5 # the upper bound of the integral
nbins <- 100 # the number of bins (or the step-size) of the integral
fit1=mle(loglike,start=list(l_sig_e=log(10),l_sig_b=log(10),beta=70),nobs=length(Streams$Density),method="BFGS")
summary(fit1)
l_sig_e=coef(fit1)[[1]]
l_sig_b=coef(fit1)[[2]]
sig_e=exp(coef(fit1)[[1]])
sig_b=exp(coef(fit1)[[2]])
beta=coef(fit1)[[3]]
AIC(fit1)
(-1)*logLik(fit1)[1] # the -1 converts to the negative log likelihood

```

Can you compare this to the estimates from `lme()` or `lme4()`?
```{r compare to to lme}
summary(fit1)
summary(lm3)
```
Let's compute a likelihood profile for the population mean density $\beta$ and compare the 95% confidence intervals from this likelihood profile with that determined using LME.

Likelihood profiling is very similar to the Grid Approximation in Lab 2, but applied to Maximum Likelihood Estimation rather than Bayesian inference. To do this, we set up a sequence of possible parameters (in our case, $\beta$ - the group-level mean density among streams) from which to calculate log-likelihoods from (conditional on the updated ML estimates of the *other* parameters).

We will re-arrange our functions accordingly.

Confidence intervals from likelihood profiling are based on the log-likelihood function.  For a given parameter, likelihood theory shows that the upper and lower points along a sequence of potential values in that parameter that fall within 1.92 units of the maximum of the log-likelihood function provide a 95% confidence interval when there is no extrabinomial variation (i.e. c = 1). The value 1.92 comes from being half of the $\chi_2$ value of 3.84 with 1 degree of freedom. Thus, the same confidence interval can also be calculated from the deviance by adding 3.84 to the minimum of the deviance function (where the deviance is the log-likelihood multiplied by -2 minus the -2 log likelihood value of the maxmum likelihood model).

Below is a likelihood profile of the beta parameter (based on a search along $\beta_{MLE}\pm0.75\beta_{MLE}$ constructed for the numerical computation of a mixed-effects model of the Streams dataset. This function calculates the negative log-likelihood profiled across fixed values for beta to assess the 95% confidence intervals, calculated as the minimum and maximum values of β for which $ln\mathcal{L}(\theta|\beta_i )\ge(ln\mathcal{L}(\theta|\beta_{MLE})-1.92)$. The 95% CI from likelihood profiling are compared to the 95% CI calculated from the ‘LME’ package using the function ‘intervals’ in Program R.

```{r profile}
b.profile=seq(from=beta-0.75*beta,to=beta+0.75*beta,length=200)
lowerb <- -5
upperb <- 5
nbins <- 100
theta=c(log(sig_e),log(sig_b))
b.pro.fun = function (theta) #(theta)
{
  beta <- b.pro # note that now the estimate of b.pro is fixed
  sig_e <- exp(theta[1]) # in this case, we will fix sig_e as well but we could re-estimate that here
  sig_b <- exp(theta[2]) # in this case, we will fix sig_e as well but we could re-estimate that here
  stream <- c(1,2,3,4,5,6)
  like.str=vector(length=length(stream))
  for (i in stream)
  {
    like.str[i] <- simpson(lowerb=lowerb, upperb=upperb, nbins=nbins, 
                        sig_e=sig_e, sig_b=sig_b, beta=beta, stream=stream[i])
  }
  nll <- (-1)*sum(log(like.str))
  return(nll)
}

beta.loglik=rep(NA,length(b.profile))
for(i in 1:length(b.profile))
{
  b.pro <- b.profile[i]
  fit.pro <- optim(theta,b.pro.fun,method="BFGS",control=list(maxit=1000))
  beta.loglik[i] <- fit.pro$value
}
df=data.frame("b.profile"=b.profile,"log-like"=beta.loglik)
plot(b.profile,beta.loglik,type="l",lty=1,lwd=2,col="black",ylab="Negative Log-Likelihood",xlab="Beta")
abline(v=df[,1][which(df[,2]==min(df[,2]))],col="red",lty=1)
x <- which((-1)*beta.loglik>=logLik(fit1)[1]-1.92) # where is the 
print(b.profile[c(min(x),max(x))]) # here are the minimum/maximum estimates of beta within the 95% CI
print(intervals(lm3))
abline(v=df[min(x),1],col="red",lty=2) #printing x and finding the minimum TRUE row number
abline(v=df[max(x),1],col="red",lty=2) #printing x and finding the maximum TRUE row number
abline(v=64.241,col="darkgreen",lty=3) #taken from the LME fits below
abline(v=93.016,col="darkgreen",lty=3) #taken from the LME fits below
legend("topright",c("ML Beta Value","Profile 95% CI","LME 95% CI"),col=c("red","red","darkgreen"),lty=c(1,2,3))
```

Overall, the confidence intervals (CI) between the two methods are very similar with the profile 95% CI of $\beta$ at 64.7-92.6 and the ‘LME’ 95% CI of $\beta$ at 64.2– 93.0. The `lme()` 95% CI are slightly wider than the likelihood profile 95% CI. In part, this is because the ‘LME’ confidence intervals are obtained from the variance-covariance matrix from all parameters (based on the diagonals of the inversed Hessian matrix +/- 1.96) rather than uncertainty in just one parameter, iteratively - but it may also be because the parameter is indeed *not* asymptotically normal.